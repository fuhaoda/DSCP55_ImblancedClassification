\documentclass[12pt]{article}
\usepackage{amsfonts}
\usepackage{amsbsy}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{rotating}
\usepackage{graphics}
\usepackage{epstopdf}
\usepackage{pdfsync}
\usepackage{natbib}
\usepackage{caption}
\usepackage{xcolor}
\usepackage{hyperref}

\textheight 9.2 in \textwidth 6.4 in
\topmargin -0.15 in       % for PC
\topmargin -0.7 in       % for Unix
\oddsidemargin 0.10in %\evensidemargin 0.0in
\parskip=.00in
\renewcommand{\baselinestretch} {1.2}
\makeatletter \setcounter{page}{1}
\def\singlespace{\def\baselinestretch{1}\@normalsize}
\def\endsinglespace{}

\@addtoreset{equation}{section}
\renewcommand{\theequation} {\arabic{section}.\arabic{equation}}
\renewcommand{\thefigure}{\arabic{figure}}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newcommand{\distas}[1]{\mathbin{\overset{#1}{\kern\z@\sim}}}%


% begin definition
\def\conas{\stackrel{a.s.} {\rightarrow}}         % conv. a.s.
\def\conP{\stackrel{\cal P} {\rightarrow}}        % conv. in probability
\def\conD{\stackrel{\cal D} {\rightsquigarrow}}        % conv. in distribution
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\def\iid{\stackrel{ iid} {\sim}}          % i.i.d
\def\hat{\widehat}
\def\tilde{\widetilde}

%general sets
\def\cal{\mathcal}
\def\calA{{\cal A}} %Action sets
\def\calB{{\cal B}} %Bounded set
\def\calC{{\cal C}} %Convex set
\def\calH{{\cal H}} %Hilbert Space
\def\calS{{\cal S}} %A regular set
\def\calNr{{\cal N}_r} %Neighborhood
\def\calX{{\cal X}} %Covariate space
\def\calF{{\cal F}} %Function space


\def\calM{{\cal M}} %Function space

%special sets
\def\bbR{{\mathbb{R}}} %Real number
\def\bbN{{\mathbb{N}}}%Natural number
\def\bbQ{{\mathbb{Q}}} %Rational number
\def\bbZ{{\mathbb{Z}}} %Integer

%Matrix
\def\bA{{\bf A}}
\def\bB{{\bf B}}
\def\bC{{\bf C}}
\def\bD{{\bf D}}
\def\bH{{\bf H}}
\def\bI{{\bf I}}
\def\bJ{{\bf J}}
\def\bP{{\bf P}}
\def\bT{{\bf T}}
\def\bW{{\bf W}}
\def\bX{{\bf X}}

%vector
\def\balpha{{\boldsymbol \alpha}}
\def\bbeta{{\boldsymbol \beta}}
\def\btheta{{\boldsymbol \theta}}

\def\bzero{{\bf 0}}
\def\bone{{\bf 1}}
\def\bbf{{\bf f}}
\def\br{{\bf r}}
\def\by{{\bf y}}

%notations
\def\sign{{\mathrm{sign}}}
\def\var{{\mathrm{var}}}
\def\cov{{\mathrm{cov}}}
\def\ind{\perp\!\!\!\perp}

%others
\def\mif{\mathrm{if}\ }
\def\ow{\mathrm{otherwise}\ }
\def\st{\mathrm{subject\ to}\quad }
\def\diag{\mathrm{diag}}
\def\minimize{\mathrm{minimize}\quad }
\def\maximize{\mathrm{maximize}\quad }
\def\dom{{\rm dom}}



\begin{document}
\title
{\bf My Data Science Challenging Problems \# 55: Imbalanced Classification}
\author
{
Drafted by Haoda Fu\\
XXX \\
XXXX \\
\textsl{xxx} }
\date{\today}

\maketitle
\begin{abstract}
\textbf{{\color[rgb]{1,0,0} If you happen to have a solution of this question, please contact Haoda Fu at \href{mailto:fu\_haoda@lilly.com }{fu\_haoda@lilly.com} or  \href{mailto:ffuhaoda@gmail.com}{fuhaoda@gmail.com}  or commit your solution to \url{https://github.com/fuhaoda/StreamingDataFeatureSelections.git} }
}

\end{abstract}

\noindent {\bf Key words and Phrases}: *** ***.

\noindent {\bf Short title}: ***
\section{Introduction}
This problem is about how to solve a classification problem when the observed positive and negative cases are imbalanced.

Let us assume our samples are representative of the population at this moment. 
Suppose we have $\{(Y_i, X_i), i=1,\cdots, n \}$ where $Y_i \in \{0,1 \}$ and $X_i$ are covariates. 
Data can be split into training and testing datasets, and we use $\calM = \{Y_m, X_m\}$ for training ($m$ stands for modeling) and use $\calH = \{Y_h, X_h\}$ as testing ($h$ stands for holdout).

As we know, the misclassification in reality often has different cost. How can we develop a classifier for a given cost for 
\begin{itemize}
	\item NPV vs PPV
	\item Sensitivity vs Specificity
\end{itemize}

To be specific, how we can estimate $D_o(\cdot)$ as a classifier built on $\calM$ to minimize the loss calculated  on $\calH$ as,
\begin{align*}
L_o & = \sum_{\calH, Y_i=0} W_p I \{D_o(X_i) \neq Y_i\}/\sum_{\calH} I\{D_0(X_i)=1\} + \sum_{\calH, Y_i=1} W_n I \{D_o(X_i) \neq Y_i\}/\sum_{\calH} I\{D_0(X_i)=0\},
\end{align*}
where $W_p$ is the loss of false positive and $W_n$ is the loss of false negative. 
The $L_o$ can be calculated by multiple random split then taking the average loss as in cross validation.

Similarly, we can also ask for a classifier $D_t(\cdot)$ to optimize the weighted cost based on sensitivity and specificity calculated on $\calH$.

To derive $D_0(\cdot)$ or $D_t(\cdot)$, we can have two approaches. One approach is based on the modified loss function and find some convex approximation. Another approach might be data augmentation approach. 

For the data augmentation approach, what is a procedure that can be generically used for any existing classification algorithms, such as XGBoost, Deep Neural Network, Random Forest?

\section{Sampling Bias}
What if we know that positive or negative cases are under sampled with a know ratio $r(X)$. How we can take this into account for estimating the classifier.

\section{Personalized Solution}
What if we want to get an optimal solution for a fixed $X$.

\section{Personalized Variable Selection}
To achieve certain level minimal accuracy, we may not need to measure all the variables. How we can measure right amount of number variables to minimize the measurement cost. 
The solution is likely a sequential variable selection solution.

\section*{Appendix: Proof of Equation ()}.




\bibliographystyle{/Users/c082213/BoxSync/Help/References/asa_HD}
\bibliography{/Users/c082213/BoxSync/Help/References/MyStats}


\end{document}
